{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elamite Word2Vec Model\n",
    "## Adapted for local use from Basic_Word2Vec notebook\n",
    "\n",
    "This notebook builds a Word2Vec model from the Elamite texts and computes cosine similarity scores.\n",
    "\n",
    "See documentation: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies (run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you need to install gensim\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries and load texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Load all text files from the texts/ directory\n",
    "texts_dir = 'texts'\n",
    "txts = []\n",
    "\n",
    "for file in os.listdir(texts_dir):\n",
    "    if file.endswith('.txt'):\n",
    "        with open(os.path.join(texts_dir, file), 'r', encoding='utf-8') as f:\n",
    "            txts.append(f.read())\n",
    "\n",
    "print(f\"Loaded {len(txts)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenize texts\n",
    "\n",
    "For Elamite texts, we use simple whitespace tokenization to preserve the transliteration format (including hyphens, parentheses, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_elamite(texts):\n",
    "    \"\"\"Tokenize Elamite texts by whitespace, preserving special characters.\"\"\"\n",
    "    tokenized = []\n",
    "    for text in texts:\n",
    "        # Split by whitespace, keeping transliteration intact\n",
    "        words = text.strip().split()\n",
    "        if words:  # Only add non-empty documents\n",
    "            tokenized.append(words)\n",
    "    return tokenized\n",
    "\n",
    "words = tokenize_elamite(txts)\n",
    "print(f\"Tokenized {len(words)} documents\")\n",
    "print(f\"Sample tokens from first document: {words[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Word2Vec model\n",
    "# min_count=1 ensures all words are included (important for small corpus)\n",
    "# vector_size=100 is the dimensionality of word vectors\n",
    "# window=5 is the context window size\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences=words,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {len(w2v.wv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with some example words\n",
    "# Find most similar words to a given word\n",
    "test_word = 'su-un-ki-ik'  # Change this to test different words\n",
    "\n",
    "if test_word in w2v.wv:\n",
    "    print(f\"Most similar to '{test_word}':\")\n",
    "    for word, score in w2v.wv.most_similar(test_word, topn=10):\n",
    "        print(f\"  {word}: {score:.4f}\")\n",
    "else:\n",
    "    print(f\"'{test_word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get similarity between two words\n",
    "word1 = 'dingir-gal'\n",
    "word2 = 'DINGIR-GAL'\n",
    "\n",
    "if word1 in w2v.wv and word2 in w2v.wv:\n",
    "    similarity = w2v.wv.similarity(word1, word2)\n",
    "    print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")\n",
    "else:\n",
    "    print(\"One or both words not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute cosine similarity for each word\n",
    "\n",
    "For each word, we compute the average cosine similarity to its most similar neighbors. This gives a measure of how \"typical\" or \"central\" each word is in the semantic space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_similarity(word, model, topn=5):\n",
    "    \"\"\"Compute average cosine similarity of a word to its top-n neighbors.\"\"\"\n",
    "    try:\n",
    "        similar = model.wv.most_similar(word, topn=topn)\n",
    "        avg_sim = np.mean([score for _, score in similar])\n",
    "        return round(avg_sim, 4)\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "# Test\n",
    "test_word = 'su-un-ki-ik'\n",
    "print(f\"Average similarity for '{test_word}': {compute_avg_similarity(test_word, w2v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Update CSV with cosine similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the original CSV and add cosine similarity column\n",
    "input_csv = 'UntN-Nasu texts Word-level.csv'\n",
    "output_csv = 'UntN-Nasu texts Word-level with similarity.csv'\n",
    "\n",
    "rows = []\n",
    "with open(input_csv, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    fieldnames = reader.fieldnames + ['Cosine_similarity']\n",
    "    \n",
    "    for row in reader:\n",
    "        word = row['Text']\n",
    "        similarity = compute_avg_similarity(word, w2v)\n",
    "        row['Cosine_similarity'] = similarity if similarity else ''\n",
    "        rows.append(row)\n",
    "\n",
    "# Write the updated CSV\n",
    "with open(output_csv, 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Updated CSV saved to: {output_csv}\")\n",
    "print(f\"Total rows: {len(rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = 'elamite_word2vec.model'\n",
    "w2v.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# To load later:\n",
    "# loaded_model = Word2Vec.load('elamite_word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install visualization dependencies\n",
    "# !pip install matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get all words and vectors\n",
    "vocab_words = list(w2v.wv.index_to_key)\n",
    "vectors = np.array([w2v.wv[word] for word in vocab_words])\n",
    "\n",
    "# Reduce to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(vectors)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.5)\n",
    "\n",
    "# Annotate some words (sample to avoid overcrowding)\n",
    "sample_indices = np.random.choice(len(vocab_words), min(50, len(vocab_words)), replace=False)\n",
    "for i in sample_indices:\n",
    "    plt.annotate(vocab_words[i], (vectors_2d[i, 0], vectors_2d[i, 1]), fontsize=8)\n",
    "\n",
    "plt.title('Elamite Word Embeddings (PCA)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.tight_layout()\n",
    "plt.savefig('word_embeddings_pca.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved to: word_embeddings_pca.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
